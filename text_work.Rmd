---
title: "Text Mining Tests"
author: "Randy Yee (pcx5@cdc.gov)"
date: "October 27, 2020"
output: html_document
---

TODOs:
1) Narratives cleaning
2) Lexicon development: stop words, sentiments, * User editable in app, user to storage
3) N-gram * User defined N

## Import libraries

```{r, echo=F}
library(tidytext)
library(tidyverse)
library(scales)
library(wordcloud)
library(reshape2)
```


## Import Narratives

```{r}
narratives_df <- read_excel("C:/Users/pcx5/Downloads/Narratives FY2020 Q4 Pre-Cleaning.xlsx", 
    skip = 6)
```


## Tokenize the words in each narrative

```{r}
test <- narratives_df %>%
  mutate(row_num = 1:n()) %>%
  unnest_tokens(word, Narrative)
head(test)
```


## Remove stop words

```{r}
#TODO Additional cleaning
#data("stop_words")
test <- test %>%
  anti_join(stop_words)
head(stop_words)
```


## Find most common words
Need to remove common HIV words

```{r}
test2 <- test %>%
  count(word, sort = T)
test2
```


## Plot word frequency

```{r}
test %>%
  filter(`Operating Unit`=="Mozambique")%>%
  count(word, sort = T) %>%
  filter(n>200) %>%
  mutate(word = reorder(word,n))%>%
  ggplot(aes(word,n))+
  geom_col()+
  xlab(NULL)+
  coord_flip()
```

## Plot fequency by OU and overall

```{r}
frequency <- test %>%
  #mutate(word = str_extract(word, "[a-z']+")) %>%
  count(`Operating Unit`, word) %>%
  group_by(`Operating Unit`) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>%
  filter(!str_detect(`Operating Unit`,"Office of U.S. Foreign Assistance Resources")) %>%
  spread(`Operating Unit`, proportion) %>%
  gather(`Operating Unit`, proportion, `Angola`:`Zimbabwe`)


all_freq <- test %>%
  #mutate(word = str_extract(word, "[a-z']+")) %>%
  filter(!str_detect(`Operating Unit`,"Office of U.S. Foreign Assistance Resources")) %>%
  count(word) %>%
  #group_by(`Operating Unit`) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n)
```

============================================================================
# 1) Sentiment analysis (BING)
Potential Options:
1) Choice of lexicon (AFINN, BING)


```{r}
positive <- get_sentiments("bing") %>%
  filter(sentiment == "positive") %>%
  filter(word != "positive") %>%
  filter(word != "positives")
```


## Filter for one OU and merge positive sentiment words

```{r}
test %>%
  filter(`Operating Unit` == "Angola") %>%
  semi_join(positive) %>%
  count(word, sort = TRUE)
```

## Look at positives 

```{r}
test %>%
  filter(`Operating Unit` == "Angola") %>%
  semi_join(positive) %>%
  count(word, sort = TRUE)
```

## Configure sentiment lexicon

```{r}
# TODO: Need to edit for PEPFAR, HIV/AIDS

bing <- get_sentiments("bing")%>%
  filter(word != "positive") %>%
  filter(word != "positives") %>%
  filter(word != "negative") %>%
  filter(word != "negatives") %>%
  filter(word != "patient")
```

## Join sentiments to narratives
Potential sentiments options:
Sentiments by OU > Indicator Bundle > Indicator
Sentiments by OU > Mech > Indicator Bundle > Indicator

```{r}
mersentiment <- test %>%
  inner_join(bing) %>%
  count(`Operating Unit`, `Indicator Bundle`, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```

## Visualize sentiments by OU

```{r}
ggplot(mersentiment, aes(`Indicator Bundle`, sentiment, fill = `Operating Unit`)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~`Operating Unit`, scales = "free_x")
```

## Find most common negative and positive words

```{r}
bing_word_counts <- test %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE)
bing_word_counts
```

## Contribution of each word to overall sentiment

```{r}
bing_word_counts %>%
  filter(n > 150) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(y = "Contribution to sentiment")
```

============================================================================
# 2) Wordcloud

```{r, fig.height=6, fig.width=6}
test2 %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

## Comparison cloud

```{r}
test %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 100)
```

============================================================================
## Sentiments by sentence

```{r}
narrative_sent <- narratives_df %>% 
  unnest_tokens(sentence, Narrative, token = "sentences")
narrative_sent$sentence[2]
```

## Normalize amount of negative using total wordcounts per narrative

```{r}
bingnegative <- get_sentiments("bing") %>%
  filter(sentiment == "negative")

wordcounts <- test %>%
  group_by_at(vars(-word)) %>%
  summarize(words = n())

test %>%
  semi_join(bingnegative) %>%
  group_by_at(vars(-word)) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("Operating Unit", "Org Level", "Organisation Site",
                               "Indicator Bundle", "Indicator", "Support Type",
                               "Funding Mechanism", "Implementing Mechanism",
                               "Fiscal Year", "Fiscal Quarter",
                               "row_num")) %>%
  mutate(ratio = negativewords/words) %>%
  top_n(1)
```

============================================================================
# 3) tf_idf

## Prepare for tf_idf
Potential if_idf options:


```{r}
narrative_words <- narratives_df %>%
  #mutate(row_num = 1:n()) %>%
  unnest_tokens(word, Narrative) %>%
  #unite("title", `Operating Unit`:`Fiscal Quarter`, sep = ";") %>%
  count(`Operating Unit`, word, sort = TRUE)

total_words <- narrative_words %>%
  group_by(`Operating Unit`) %>%
  summarize(total = sum(n))

narrative_words <- left_join(narrative_words, total_words)

narrative_words
```

## Histogram of word frequencies

```{r}
ggplot(narrative_words, aes(n/total, fill = `Operating Unit`)) +
  geom_histogram(show.legend = F) +
  facet_wrap(~`Operating Unit`, scales = "free_y")
```

## Ranking word frequencies

```{r}
freq_by_rank <- narrative_words %>%
  group_by(`Operating Unit`) %>%
  mutate(
    rank = row_number(),
    `term frequency` = n/total
  )

freq_by_rank
```

## Plot word frequencies

```{r}
freq_by_rank %>%
  ggplot(aes(rank, `term frequency`, color = `Operating Unit`)) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = F) +
  scale_x_log10() +
  scale_y_log10()
```

## Test Zipf's law assumption

```{r}
rank_subset <- freq_by_rank %>%
  filter(rank < 500, rank > 10)

lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
```

## Plot word frequencies with linear coefficients

```{r}
freq_by_rank %>%
  ggplot(aes(rank, `term frequency`, color = `Operating Unit`)) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = F) +
  scale_x_log10() +
  scale_y_log10() +
  geom_abline(intercept = -0.9355, slope = -0.9322, linetype = 2)
```

## Use bind_tf_idf to get tf_idf parameters

```{r}
narrative_words <- narrative_words %>%
  bind_tf_idf(word, `Operating Unit`, n)

narrative_words
```

## Arrange tf_idf results decreasing

```{r}
narrative_words %>%
  arrange(desc(tf_idf))
```

============================================================================
# 4) N-grams and correlations

## Tokenize by n-gram

```{r}
narratives_bigrams <- narratives_df %>%
  unnest_tokens(bigram, Narrative, token = "ngrams", n = 2)

narratives_bigrams
```

## Explore n-grams

```{r}
narratives_bigrams %>%
  count(bigram, sort = T)
```

## Remove stop words
TODO: need to remove common bigrams like "reporting period"

```{r}
bigrams_separated <- narratives_bigrams %>%
  separate(bigram, c("word1", "word2", sep = " "))

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)%>%
  filter(!is.na(word1) & !is.na(word2))

bigram_counts <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, sort = T)

bigram_counts
```

## Other n's
TODO: Implement for app user

```{r}
m = 3 #
word_list <- mapply(function(y) paste0("word", y), 1:m)
#word_list <- append(word_list, ", sep = \" \" ")
narratives_ngrams <- narratives_df %>%
  filter(`Operating Unit` != "\nOffice of U.S. Foreign Assistance Resources") %>%
  unnest_tokens(ngram, Narrative, token = "ngrams", n = m) %>%
  separate(ngram, word_list)

narratives_ngrams
```

## N-gram analysis

```{r}
covid_words <- c("covid", "coronavirus", "covid-19")

bigrams_filtered %>%
  filter(word2 %in% covid_words) %>%
  count(`Operating Unit`, word1, sort = T) 

bigrams_filtered %>%
  filter(word1 %in% covid_words) %>%
  count(`Operating Unit`, word2, sort = T) %>%
  filter(word2 != "19") #remove common second word "19"
```

## tf_idf use

```{r}
bigram_tf_idf <- bigrams_filtered %>%
  filter(word2 %in% covid_words) %>%
  unite(bigram, "word1", "word2", sep = " ") %>%
  count(`Operating Unit`, bigram) %>%
  bind_tf_idf(bigram, `Operating Unit`, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf
```

```{r}
narratives_ngrams %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word) %>%
  filter(!is.na(word1) & !is.na(word2) & !is.na(word3)) %>%
  filter(word2 %in% covid_words) %>%
  unite(bigram, "word1", "word2", "word3", sep = " ") %>%
  count(`Operating Unit`, bigram) %>%
  bind_tf_idf(bigram, `Operating Unit`, n) %>%
  arrange(desc(tf_idf))
```


## Bigrams for sentiment analysis (Using AFINN)

```{r}
AFINN <- get_sentiments("afinn")

AFINN
```


```{r}

not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, value, sort = TRUE)

bigrams_separated %>%
  filter(word1 %in% c("not", "no")) %>%
  count(word1, word2, sort = T)
```

```{r}
trigrams_separated <- narratives_df %>%
  unnest_tokens(ngram, Narrative, token = "ngrams", n = 4) %>%
  separate(ngram, c("word1", "word2", "word3", "word4", sep = " "))%>%
  filter(word1 %in% c("not", "no")) %>%
  count(word1, word2, word3, word4, sort = T)

trigrams_separated
```

```{r}
bigrams_filtered %>%
  filter(word2 == "street") %>%
  count(book, word1, sort = TRUE)
```


============================================================================
# 5) LIWC

```{r}
icpi_covid_list <- c(
  "covid19",
  "covid",
  "lockdown",
  "social",
  "distancing",
  "distance",
  "isolation",
  "isolate",
  "quarantine" 
)

liwc_summary <- narratives_df %>%
  mutate(row_num = 1:n()) %>%
  unnest_tokens(word, Narrative) %>%
  mutate(covid = if_else(word %in% icpi_covid_list, 1, 0)) %>%
  group_by_at(vars(-word, -covid)) %>%
  summarise(total = n(),
            covid_n = sum(covid)) %>%
  ungroup() %>%
  mutate(covid_per = covid_n/ total)
  
liwc_summary
```

============================================================================
# 6) VL Table Analysis

```{r}

```

